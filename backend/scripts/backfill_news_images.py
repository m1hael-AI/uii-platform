"""
backfill_news_images.py

–ü—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ –≤—Å–µ–º –Ω–æ–≤–æ—Å—Ç—è–º –∏ –ø—ã—Ç–∞–µ—Ç—Å—è –¥–æ—Å—Ç–∞—Ç—å –ª—É—á—à–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ source_urls.
–ü–µ—Ä–µ–±–∏—Ä–∞–µ—Ç og:image + twitter:image, —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –º—É—Å–æ—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã.

–ó–∞–ø—É—Å–∫:
    docker exec ai_university_backend python scripts/backfill_news_images.py
"""

import sys
import os
import asyncio
import re

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import httpx
from loguru import logger
from sqlalchemy import select
from database import async_session_factory
from models import NewsItem

# ---- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----
REQUEST_TIMEOUT = 6.0
DELAY_BETWEEN = 0.3
USER_AGENT = "Mozilla/5.0 (compatible; UII-Backfiller/1.0)"

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ—Ç—Å–µ–≤–∞ –º—É—Å–æ—Ä–Ω—ã—Ö –∫–∞—Ä—Ç–∏–Ω–æ–∫
_BAD_PATTERNS = re.compile(
    r"(logo|icon|favicon|avatar|pixel|spacer|1x1|badge|banner_small|placeholder|default[-_]img|\.gif)",
    re.IGNORECASE,
)

# og:image ‚Äî property –ø–µ—Ä–µ–¥ content –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç
_OG_RE = [
    re.compile(r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\'](https?://[^"\'>\s]+)["\']', re.IGNORECASE),
    re.compile(r'<meta[^>]+content=["\'](https?://[^"\'>\s]+)["\'][^>]+property=["\']og:image["\']', re.IGNORECASE),
]
# twitter:image
_TW_RE = [
    re.compile(r'<meta[^>]+name=["\']twitter:image["\'][^>]+content=["\'](https?://[^"\'>\s]+)["\']', re.IGNORECASE),
    re.compile(r'<meta[^>]+content=["\'](https?://[^"\'>\s]+)["\'][^>]+name=["\']twitter:image["\']', re.IGNORECASE),
]


def _extract_best_image(html: str) -> str | None:
    """
    –ò—â–µ—Ç og:image –∏ twitter:image, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—ã–π –ø—Ä–æ—à–µ–¥—à–∏–π —Ñ–∏–ª—å—Ç—Ä.
    –ü–æ—Ä—è–¥–æ–∫: og:image ‚Üí twitter:image.
    """
    candidates = []
    for patterns in (_OG_RE, _TW_RE):
        for p in patterns:
            m = p.search(html)
            if m:
                candidates.append(m.group(1).strip())
                break  # –Ω–∞—à–ª–∏ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞, –¥–∞–ª—å—à–µ –Ω–µ –∏—â–µ–º

    for url in candidates:
        if not _BAD_PATTERNS.search(url):
            return url

    # –ï—Å–ª–∏ –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –º—É—Å–æ—Ä–Ω—ã–µ (logo, .gif –∏ —Ç.–¥.) - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None, —á—Ç–æ–±—ã —Å–∫—Ä–∏–ø—Ç –ø–µ—Ä–µ—à–µ–ª –∫ —Å–ª–µ–¥—É—é—â–µ–º—É URL
    return None


async def fetch_og_image(url: str, client: httpx.AsyncClient) -> str | None:
    try:
        r = await client.get(url, timeout=REQUEST_TIMEOUT, follow_redirects=True)
        if r.status_code != 200:
            return None
        return _extract_best_image(r.text)
    except Exception as e:
        logger.warning(f"  ‚ö†Ô∏è  GET {url[:80]} failed: {e}")
    return None


async def backfill():
    logger.info("üöÄ Starting news image backfill (overwrite mode, improved filtering)...")

    async with async_session_factory() as db:
        # –í—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ —Ç–æ–∂–µ)
        result = await db.execute(select(NewsItem).order_by(NewsItem.id))
        news_list = result.scalars().all()

    total = len(news_list)
    logger.info(f"üì∞ Found {total} news items total (will overwrite existing images)")

    if total == 0:
        logger.info("‚úÖ Nothing to do.")
        return

    found = 0
    skipped = 0

    async with httpx.AsyncClient(headers={"User-Agent": USER_AGENT}) as client:
        for i, news in enumerate(news_list, 1):
            urls = news.source_urls or []
            if not urls:
                logger.info(f"[{i}/{total}] ID={news.id} ‚Äî no source_urls, skip")
                skipped += 1
                continue

            logger.info(f"[{i}/{total}] ID={news.id} ‚Äî trying {len(urls)} URL(s)...")
            image_url = None

            for url in urls:
                image_url = await fetch_og_image(url, client)
                if image_url:
                    logger.info(f"  ‚úÖ Found: {image_url[:100]}")
                    break
                await asyncio.sleep(DELAY_BETWEEN)

            if image_url:
                async with async_session_factory() as db:
                    item = await db.get(NewsItem, news.id)
                    if item:
                        item.image_url = image_url
                        await db.commit()
                found += 1
            else:
                logger.info(f"  ‚ùå No image found for ID={news.id}")
                skipped += 1

            await asyncio.sleep(DELAY_BETWEEN)

    logger.info(
        f"\nüèÅ Done. Updated: {found}/{total}, no image found: {skipped}/{total}"
    )


if __name__ == "__main__":
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(backfill())
