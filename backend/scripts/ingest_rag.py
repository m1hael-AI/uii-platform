import asyncio
import os
import sys
from typing import List
from datetime import datetime

# Add parent directory to path to import models and config
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from sqlalchemy import select, delete
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import sessionmaker
from loguru import logger
from dotenv import load_dotenv

# Imports
import re
import nltk
from nltk.tokenize import sent_tokenize

# Ensure NLTK data is downloaded
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords', quiet=True)

# Project imports
from database import async_engine
from models import WebinarLibrary, WebinarChunk
from services.openai_service import generate_embedding, generate_embeddings_batch

load_dotenv()

# Setup Logging
logger.add("ingest_rag.log", rotation="10 MB")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# RAG INGESTION CONFIG ‚Äî –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞—Ä–µ–∑–∫–∏ –∑–¥–µ—Å—å
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
RAG_INGEST_CONFIG = {
    "chunk_size_chars": 800,   # –¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö (~5-6 VTT-–±–ª–æ–∫–æ–≤, ~1-2 –º–∏–Ω)
    "overlap_blocks":   1,     # –ë–ª–æ–∫–æ–≤ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è (–±—ã–ª–æ 2, —É–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Å–Ω–∏–∂–µ–Ω–∏—è –¥—É–±–ª–µ–π)
    "batch_size":       50,    # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (–í–µ—Ä–Ω—É–ª–∏ 50, —Ç–∞–∫ –∫–∞–∫ –ø–æ—Ñ–∏–∫—Å–∏–ª–∏ —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤)
}

async def get_db_session() -> AsyncSession:
    async_session = sessionmaker(
        async_engine, class_=AsyncSession, expire_on_commit=False
    )
    return async_session()

def parse_vtt_blocks(text: str) -> List[str]:
    """
    –ü–∞—Ä—Å–∏—Ç VTT-—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –≤ —Å–ø–∏—Å–æ–∫ –∞—Ç–æ–º–∞—Ä–Ω—ã—Ö –±–ª–æ–∫–æ–≤.
    –ö–∞–∂–¥—ã–π –±–ª–æ–∫ ‚Äî —Å—Ç—Ä–æ–∫–∞ –≤–∏–¥–∞ "HH:MM:SS --> HH:MM:SS\n–¢–µ–∫—Å—Ç —Ä–µ—á–∏".
    """
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫.
    # –ü–æ—Ä—è–¥–æ–∫ –≤–∞–∂–µ–Ω: —Å–Ω–∞—á–∞–ª–∞ –ª–∏—Ç–µ—Ä–∞–ª—å–Ω—ã–µ escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (4 —Å–∏–º–≤–æ–ª–∞: \r\n),
    # –ø–æ—Ç–æ–º —Ä–µ–∞–ª—å–Ω—ã–µ –±–∞–π—Ç—ã CRLF (2 –±–∞–π—Ç–∞). –û–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    # –æ—Ç —Ç–æ–≥–æ, –∫–∞–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –±—ã–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤ –ë–î.
    text = text.replace('\\r\\n', '\n').replace('\\r', '\n').replace('\\n', '\n')
    text = text.replace('\r\n', '\n').replace('\r', '\n')
    
    # 1. –ü—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π VTT: "00:00:00.000 --> 00:00:00.000"
    ts_pattern_vtt = re.compile(
        r'(\d{2}:\d{2}:\d{2}\.\d{3}\s*-->\s*\d{2}:\d{2}:\d{2}\.\d{3})'
    )
    positions = [(m.start(), m.end()) for m in ts_pattern_vtt.finditer(text)]
    
    # 2. –ï—Å–ª–∏ VTT –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –≤ —Å–∫–æ–±–∫–∞—Ö [MM:SS...]
    # –°–¥–µ–ª–∞–ª–∏ —Ä–µ–≥—É–ª—è—Ä–∫—É –±–æ–ª–µ–µ "–∂–∞–¥–Ω–æ–π" –∏ –≥–∏–±–∫–æ–π –∫ –ø—Ä–æ–±–µ–ª–∞–º
    if not positions:
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω [—Ü–∏—Ñ—Ä—ã:—Ü–∏—Ñ—Ä—ã ... ]:
        ts_pattern_brackets = re.compile(
            r'\[(\d{1,2}:\d{2}.*?)\]:'
        )
        positions = [(m.start(), m.end()) for m in ts_pattern_brackets.finditer(text)]

    blocks = []
    for i, (start, end) in enumerate(positions):
        timestamp = text[start:end].strip()
        content_end = positions[i + 1][0] if i + 1 < len(positions) else len(text)
        content = text[end:content_end].strip()
        if content:
            blocks.append(f"{timestamp}\n{content}")
    
    return blocks


def chunk_text_vtt(
    text: str,
    chunk_size: int = RAG_INGEST_CONFIG["chunk_size_chars"],
    overlap_blocks: int = RAG_INGEST_CONFIG["overlap_blocks"],
) -> List[str]:
    """
    –†–µ–∂–µ—Ç VTT-—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –Ω–∞ —á–∞–Ω–∫–∏ —Å—Ç—Ä–æ–≥–æ –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º timestamp-–±–ª–æ–∫–æ–≤.
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —á—Ç–æ —á–∞–Ω–∫–∏ –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞—é—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ –±–ª–æ–∫–∞.
    –î–æ–±–∞–≤–ª—è–µ—Ç overlap_blocks –±–ª–æ–∫–æ–≤ –∏–∑ –∫–æ–Ω—Ü–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —á–∞–Ω–∫–∞.
    Fallback –Ω–∞ NLTK –µ—Å–ª–∏ VTT-—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞.
    """
    blocks = parse_vtt_blocks(text)
    
    if not blocks:
        logger.error(
            f"‚ö†Ô∏è FALLBACK: VTT-–±–ª–æ–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤), "
            f"–ø—Ä–∏–º–µ–Ω–µ–Ω–∞ NLTK-–Ω–∞—Ä–µ–∑–∫–∞. –ü—Ä–æ–≤–µ—Ä—å —Ñ–æ—Ä–º–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞!"
        )
        return chunk_text_nltk(text)
    
    chunks = []
    current_blocks: List[str] = []
    current_size = 0
    
    for block in blocks:
        block_size = len(block)
        
        # SAFETY CHECK: –ï—Å–ª–∏ –±–ª–æ–∫ –≥–∏–≥–∞–Ω—Ç—Å–∫–∏–π (> 4000 —Å–∏–º–≤–æ–ª–æ–≤), —Ä—É–±–∏–º –µ–≥–æ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ
        # –≠—Ç–æ –∑–∞—â–∏—Ç–∏—Ç –æ—Ç –æ—à–∏–±–æ–∫ OpenAI (8192 —Ç–æ–∫–µ–Ω–∞), –µ—Å–ª–∏ —Ç–∞–π–º–∫–æ–¥—ã –≤–¥—Ä—É–≥ —Ä–µ–¥–∫–∏–µ –∏–ª–∏ –ø—Ä–æ–ø—É—â–µ–Ω—ã
        if block_size > 4000:
            # SAFETY CHECK: –ï—Å–ª–∏ –±–ª–æ–∫ –æ–≥—Ä–æ–º–Ω—ã–π (>4000), —Ä—É–±–∏–º –µ–≥–æ –Ω–∞ —á–∞—Å—Ç–∏, 
            # —Å—Ç–∞—Ä–∞—è—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≥—Ä–∞–Ω–∏—Ü—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (—Ç–æ—á–∫–∞, ?, ! + –ø—Ä–æ–±–µ–ª)
            
            # 1. –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = re.split(r'(?<=[.!?])\s+', block)
            
            sub_chunk = []
            sub_chunk_size = 0
            
            for sentence in sentences:
                sent_len = len(sentence)
                
                # –ï—Å–ª–∏ —Å–∞–º–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≥–∏–≥–∞–Ω—Ç—Å–∫–æ–µ (> chunk_size * 2), —Ä—É–±–∏–º –∂–µ—Å—Ç–∫–æ
                # (–∑–∞—â–∏—Ç–∞ –æ—Ç "–≤–∏—Å—è—â–∏—Ö" –∫—É—Å–∫–æ–≤ –∫–æ–¥–∞ –∏–ª–∏ –ª–æ–≥–æ–≤ –±–µ–∑ —Ç–æ—á–µ–∫)
                if sent_len > chunk_size * 2:
                    # –°–Ω–∞—á–∞–ª–∞ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–µ
                    if sub_chunk:
                        chunks.append(' '.join(sub_chunk))
                        sub_chunk = []
                        sub_chunk_size = 0
                    
                    # –†—É–±–∏–º –¥–ª–∏–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ chunk_size
                    for part in range(0, sent_len, chunk_size):
                        chunks.append(sentence[part : part + chunk_size])
                    continue

                # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø—Ä–µ–≤—ã—Å–∏—Ç chunk_size
                if sub_chunk_size + sent_len > chunk_size and sub_chunk:
                    chunks.append(' '.join(sub_chunk))
                    sub_chunk = []
                    sub_chunk_size = 0
                
                sub_chunk.append(sentence)
                sub_chunk_size += sent_len
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞—Ç–æ–∫
            if sub_chunk:
                chunks.append(' '.join(sub_chunk))
            
            continue # –ë–ª–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∏ –Ω–∞—Ä–µ–∑–∞–Ω –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º/–∫—É—Å–∫–∞–º

        if current_size + block_size > chunk_size and current_blocks:
            chunks.append('\n\n'.join(current_blocks))
            # Overlap: –ø–µ—Ä–≤—ã–µ –±–ª–æ–∫–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ —á–∞–Ω–∫–∞ = –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –±–ª–æ–∫–æ–≤ —Ç–µ–∫—É—â–µ–≥–æ
            current_blocks = current_blocks[-overlap_blocks:] if overlap_blocks > 0 else []
            current_size = sum(len(b) for b in current_blocks)
        
        current_blocks.append(block)
        current_size += block_size
    
    if current_blocks:
        chunks.append('\n\n'.join(current_blocks))
    
    return chunks


def chunk_text_nltk(text: str, chunk_size: int = 1000) -> List[str]:
    """Fallback: NLTK-–Ω–∞—Ä–µ–∑–∫–∞ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ—Å–ª–∏ VTT –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω)."""
    sentences = sent_tokenize(text, language="russian")
    chunks, current_chunk = [], ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) <= chunk_size:
            current_chunk += sentence + " "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence + " "
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

async def ingest_webinars():
    logger.info(f"üöÄ Starting RAG Ingestion (Batch Size: {RAG_INGEST_CONFIG['batch_size']}, Overlap: {RAG_INGEST_CONFIG['overlap_blocks']})...")
    
    async with await get_db_session() as db:
        # 1. Fetch all webinars with transcripts
        logger.info("Fetching webinars...")
        result = await db.execute(select(WebinarLibrary).where(WebinarLibrary.transcript_context != None))
        webinars = result.scalars().all()
        
        logger.info(f"Found {len(webinars)} webinars with transcripts.")
        
        total_chunks_created = 0
        
        for webinar in webinars:
            if not webinar.transcript_context.strip():
                continue
            
            # Skip already successfully processed webinars (1-39)
            if webinar.id not in [41, 50, 55]:
                # logger.info(f"Skipping webinar {webinar.id} (already ingested or not targeted)")
                continue

            logger.info(f"Processing webinar: {webinar.title} (ID: {webinar.id})")
            
            # 2. Cleanup old chunks for this webinar (to avoid duplicates on rerun)
            await db.execute(delete(WebinarChunk).where(WebinarChunk.webinar_id == webinar.id))
            
            # 3. Chunking –ø–æ VTT-–±–ª–æ–∫–∞–º (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã ‚Äî RAG_INGEST_CONFIG –≤—ã—à–µ)
            text = webinar.transcript_context
            chunks_text = chunk_text_vtt(text)
            logger.info(f"  -> Split into {len(chunks_text)} VTT-chunks.")
            
            # 4. Embedding & Saving (Batch Mode)
            webinar_chunks = []
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –ø–æ RAG_INGEST_CONFIG["batch_size"]
            batch_size = RAG_INGEST_CONFIG["batch_size"]
            total_chunks = len(chunks_text)
            
            for i in range(0, total_chunks, batch_size):
                batch_texts = chunks_text[i : i + batch_size]
                current_batch_indices = range(i, i + len(batch_texts))
                
                try:
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä—ã –ø–∞—á–∫–æ–π (1 –∑–∞–ø—Ä–æ—Å –≤–º–µ—Å—Ç–æ 50)
                    vectors = await generate_embeddings_batch(batch_texts)
                    
                    for text_content, vector, idx in zip(batch_texts, vectors, current_batch_indices):
                        db_chunk = WebinarChunk(
                            webinar_id=webinar.id,
                            content=text_content,
                            embedding=vector,
                            chunk_metadata={
                                "index": idx,
                                "source": "transcript",
                                "title": webinar.title
                            }
                        )
                        db.add(db_chunk)
                    
                    total_chunks_created += len(batch_texts)
                    logger.info(f"    -> Processed batch {i}-{i+len(batch_texts)}/{total_chunks}")
                    
                except Exception as e:
                    logger.error(f"  ‚ùå Failed to embed batch {i}: {e}")

            # Commit per webinar to save progress
            await db.commit()
            logger.info(f"  ‚úÖ Saved {total_chunks} chunks for '{webinar.title}'")
            
    logger.info(f"üéâ Ingestion Complete! Total chunks: {total_chunks_created}")

if __name__ == "__main__":
    # Ensure NLTK data is available (if not in docker)
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
        
    asyncio.run(ingest_webinars())
