import asyncio
import os
import sys
from typing import List
from datetime import datetime

# Add parent directory to path to import models and config
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from sqlalchemy import select, delete
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import sessionmaker
from loguru import logger
from dotenv import load_dotenv

# Imports
import re
import nltk
from nltk.tokenize import sent_tokenize

# Ensure NLTK data is downloaded
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords', quiet=True)

# Project imports
from database import async_engine
from models import WebinarLibrary, WebinarChunk
from services.openai_service import generate_embedding

load_dotenv()

# Setup Logging
logger.add("ingest_rag.log", rotation="10 MB")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# RAG INGESTION CONFIG ‚Äî –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞—Ä–µ–∑–∫–∏ –∑–¥–µ—Å—å
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
RAG_INGEST_CONFIG = {
    "chunk_size_chars": 800,   # –¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö (~5-6 VTT-–±–ª–æ–∫–æ–≤, ~1-2 –º–∏–Ω)
    "overlap_blocks":   2,     # –ë–ª–æ–∫–æ–≤ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ —á–∞–Ω–∫–∞–º–∏
}

async def get_db_session() -> AsyncSession:
    async_session = sessionmaker(
        async_engine, class_=AsyncSession, expire_on_commit=False
    )
    return async_session()

def parse_vtt_blocks(text: str) -> List[str]:
    """
    –ü–∞—Ä—Å–∏—Ç VTT-—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –≤ —Å–ø–∏—Å–æ–∫ –∞—Ç–æ–º–∞—Ä–Ω—ã—Ö –±–ª–æ–∫–æ–≤.
    –ö–∞–∂–¥—ã–π –±–ª–æ–∫ ‚Äî —Å—Ç—Ä–æ–∫–∞ –≤–∏–¥–∞ "HH:MM:SS --> HH:MM:SS\n–¢–µ–∫—Å—Ç —Ä–µ—á–∏".
    """
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫.
    # –ü–æ—Ä—è–¥–æ–∫ –≤–∞–∂–µ–Ω: —Å–Ω–∞—á–∞–ª–∞ –ª–∏—Ç–µ—Ä–∞–ª—å–Ω—ã–µ escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (4 —Å–∏–º–≤–æ–ª–∞: \r\n),
    # –ø–æ—Ç–æ–º —Ä–µ–∞–ª—å–Ω—ã–µ –±–∞–π—Ç—ã CRLF (2 –±–∞–π—Ç–∞). –û–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    # –æ—Ç —Ç–æ–≥–æ, –∫–∞–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –±—ã–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤ –ë–î.
    text = text.replace('\\r\\n', '\n').replace('\\r', '\n').replace('\\n', '\n')
    text = text.replace('\r\n', '\n').replace('\r', '\n')
    
    ts_pattern = re.compile(
        r'(\d{2}:\d{2}:\d{2}\.\d{3}\s*-->\s*\d{2}:\d{2}:\d{2}\.\d{3})'
    )
    positions = [(m.start(), m.end()) for m in ts_pattern.finditer(text)]
    
    blocks = []
    for i, (start, end) in enumerate(positions):
        timestamp = text[start:end].strip()
        content_end = positions[i + 1][0] if i + 1 < len(positions) else len(text)
        content = text[end:content_end].strip()
        if content:
            blocks.append(f"{timestamp}\n{content}")
    
    return blocks


def chunk_text_vtt(
    text: str,
    chunk_size: int = RAG_INGEST_CONFIG["chunk_size_chars"],
    overlap_blocks: int = RAG_INGEST_CONFIG["overlap_blocks"],
) -> List[str]:
    """
    –†–µ–∂–µ—Ç VTT-—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –Ω–∞ —á–∞–Ω–∫–∏ —Å—Ç—Ä–æ–≥–æ –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º timestamp-–±–ª–æ–∫–æ–≤.
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —á—Ç–æ —á–∞–Ω–∫–∏ –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞—é—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ –±–ª–æ–∫–∞.
    –î–æ–±–∞–≤–ª—è–µ—Ç overlap_blocks –±–ª–æ–∫–æ–≤ –∏–∑ –∫–æ–Ω—Ü–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —á–∞–Ω–∫–∞.
    Fallback –Ω–∞ NLTK –µ—Å–ª–∏ VTT-—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞.
    """
    blocks = parse_vtt_blocks(text)
    
    if not blocks:
        logger.error(
            f"‚ö†Ô∏è FALLBACK: VTT-–±–ª–æ–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤), "
            f"–ø—Ä–∏–º–µ–Ω–µ–Ω–∞ NLTK-–Ω–∞—Ä–µ–∑–∫–∞. –ü—Ä–æ–≤–µ—Ä—å —Ñ–æ—Ä–º–∞—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞!"
        )
        return chunk_text_nltk(text)
    
    chunks = []
    current_blocks: List[str] = []
    current_size = 0
    
    for block in blocks:
        block_size = len(block)
        
        if current_size + block_size > chunk_size and current_blocks:
            chunks.append('\n\n'.join(current_blocks))
            # Overlap: –ø–µ—Ä–≤—ã–µ –±–ª–æ–∫–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ —á–∞–Ω–∫–∞ = –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –±–ª–æ–∫–æ–≤ —Ç–µ–∫—É—â–µ–≥–æ
            current_blocks = current_blocks[-overlap_blocks:] if overlap_blocks > 0 else []
            current_size = sum(len(b) for b in current_blocks)
        
        current_blocks.append(block)
        current_size += block_size
    
    if current_blocks:
        chunks.append('\n\n'.join(current_blocks))
    
    return chunks


def chunk_text_nltk(text: str, chunk_size: int = 1000) -> List[str]:
    """Fallback: NLTK-–Ω–∞—Ä–µ–∑–∫–∞ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ—Å–ª–∏ VTT –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω)."""
    sentences = sent_tokenize(text, language="russian")
    chunks, current_chunk = [], ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) <= chunk_size:
            current_chunk += sentence + " "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence + " "
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

async def ingest_webinars():
    logger.info("üöÄ Starting RAG Ingestion...")
    
    async with await get_db_session() as db:
        # 1. Fetch all webinars with transcripts
        logger.info("Fetching webinars...")
        result = await db.execute(select(WebinarLibrary).where(WebinarLibrary.transcript_context != None))
        webinars = result.scalars().all()
        
        logger.info(f"Found {len(webinars)} webinars with transcripts.")
        
        total_chunks_created = 0
        
        for webinar in webinars:
            if not webinar.transcript_context.strip():
                continue
                
            logger.info(f"Processing webinar: {webinar.title} (ID: {webinar.id})")
            
            # 2. Cleanup old chunks for this webinar (to avoid duplicates on rerun)
            await db.execute(delete(WebinarChunk).where(WebinarChunk.webinar_id == webinar.id))
            
            # 3. Chunking –ø–æ VTT-–±–ª–æ–∫–∞–º (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã ‚Äî RAG_INGEST_CONFIG –≤—ã—à–µ)
            text = webinar.transcript_context
            chunks_text = chunk_text_vtt(text)
            logger.info(f"  -> Split into {len(chunks_text)} VTT-chunks.")
            
            # 4. Embedding & Saving
            webinar_chunks = []
            for i, chunk_content in enumerate(chunks_text):
                # Generate Embedding
                # Note: This calls OpenAI API, cost money!
                try:
                    vector = await generate_embedding(chunk_content)
                    
                    db_chunk = WebinarChunk(
                        webinar_id=webinar.id,
                        content=chunk_content,
                        embedding=vector,
                        chunk_metadata={
                            "index": i,
                            "source": "transcript",
                            "title": webinar.title
                        }
                    )
                    db.add(db_chunk)
                    total_chunks_created += 1
                except Exception as e:
                    logger.error(f"  ‚ùå Failed to embed chunk {i}: {e}")
            
            # Commit per webinar to save progress
            await db.commit()
            logger.info(f"  ‚úÖ Saved {len(chunks_text)} chunks for '{webinar.title}'")
            
    logger.info(f"üéâ Ingestion Complete! Total chunks: {total_chunks_created}")

if __name__ == "__main__":
    # Ensure NLTK data is available (if not in docker)
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
        
    asyncio.run(ingest_webinars())
